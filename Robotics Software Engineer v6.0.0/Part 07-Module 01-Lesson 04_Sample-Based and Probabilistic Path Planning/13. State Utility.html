<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   State Utility
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Sample-Based and Probabilistic Path Planning
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introduction to Sample-Based &amp; Probabilistic Path Planning.html">
       01. Introduction to Sample-Based &amp; Probabilistic Path Planning
      </a>
     </li>
     <li class="">
      <a href="02. Why Sample-Based Planning.html">
       02. Why Sample-Based Planning
      </a>
     </li>
     <li class="">
      <a href="03. Weakening Requirements.html">
       03. Weakening Requirements
      </a>
     </li>
     <li class="">
      <a href="04. Sample-Based Planning.html">
       04. Sample-Based Planning
      </a>
     </li>
     <li class="">
      <a href="05. Probabilistic Roadmap (PRM).html">
       05. Probabilistic Roadmap (PRM)
      </a>
     </li>
     <li class="">
      <a href="06. Rapidly Exploring Random Tree Method (RRT).html">
       06. Rapidly Exploring Random Tree Method (RRT)
      </a>
     </li>
     <li class="">
      <a href="07. Path Smoothing.html">
       07. Path Smoothing
      </a>
     </li>
     <li class="">
      <a href="08. Overall Concerns.html">
       08. Overall Concerns
      </a>
     </li>
     <li class="">
      <a href="09. Sample-Based Planning Wrap-Up.html">
       09. Sample-Based Planning Wrap-Up
      </a>
     </li>
     <li class="">
      <a href="10. Introduction to Probabilistic Path Planning.html">
       10. Introduction to Probabilistic Path Planning
      </a>
     </li>
     <li class="">
      <a href="11. Markov Decision Process.html">
       11. Markov Decision Process
      </a>
     </li>
     <li class="">
      <a href="12. Policies.html">
       12. Policies
      </a>
     </li>
     <li class="">
      <a href="13. State Utility.html">
       13. State Utility
      </a>
     </li>
     <li class="">
      <a href="14. Value Iteration Algorithm.html">
       14. Value Iteration Algorithm
      </a>
     </li>
     <li class="">
      <a href="15. Probabilistic Path Planning Wrap-Up.html">
       15. Probabilistic Path Planning Wrap-Up
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          13. State Utility
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="state-utility">
          State Utility
         </h1>
         <h2 id="definition">
          Definition
         </h2>
         <p>
          The
          <strong>
           utility of a state
          </strong>
          (otherwise known as the
          <strong>
           state-value
          </strong>
          ) represents how attractive the state is with respect to the goal. Recall that for each state, the state-value function yields the expected return, if the agent (robot) starts in that state and then follows the policy for all time steps. In mathematical notation, this can be represented as so:
         </p>
         <div class="mathquill">
          U^{\pi}(s) = E[\sum_{t=0}^{\infty}R(s_t)|\pi , s_0 = s]
         </div>
         <p>
          The notation used in path planning differs slightly from what you saw in Reinforcement Learning. But the result is identical.
         </p>
         <p>
          Here,
         </p>
         <ul>
          <li>
           <span class="mathquill ud-math">
            U^{\pi}(s)
           </span>
           represents the utility of a state
           <span class="mathquill ud-math">
            s
           </span>
           ,
          </li>
          <li>
           <span class="mathquill ud-math">
            E
           </span>
           represents the
           <em>
            expected
           </em>
           value, and
          </li>
          <li>
           <span class="mathquill ud-math">
            R(s)
           </span>
           represents the reward for state
           <span class="mathquill ud-math">
            s
           </span>
           .
          </li>
         </ul>
         <p>
          The utility of a state is the sum of the rewards that an agent would encounter if it started at that state and followed the policy to the goal.
         </p>
         <h2 id="calculation">
          Calculation
         </h2>
         <p>
          We can break the equation down, to further understand it.
         </p>
         <div class="mathquill">
          U^{\pi}(s) = E[\sum_{t=0}^{\infty}R(s_t)|\pi , s_0 = s]
         </div>
         <p>
          Let’s start by breaking up the summation and explicitly adding all states.
         </p>
         <div class="mathquill">
          U^{\pi}(s) = E[R(s_0) + R(s_1) + R(s_2) + ... \ |\pi , s_0 = s]
         </div>
         <p>
          Then, we can pull out the first term. The expected reward for the first state is independent of the policy. While the expected reward of all future states (those between the state and the goal) depend on the policy.
         </p>
         <div class="mathquill">
          U^{\pi}(s) = E[R(s_0)|s_0 = s] + E[R(s_1) + R(s_2) + ... \ |\pi]
         </div>
         <p>
          Re-arranging the equation results in the following. (Recall that the prime symbol, as on
          <span class="mathquill ud-math">
           s'
          </span>
          , represents the next state - like
          <span class="mathquill ud-math">
           s_2
          </span>
          would be to
          <span class="mathquill ud-math">
           s_1
          </span>
          ).
         </p>
         <div class="mathquill">
          U^{\pi}(s) = R(s) + E[\sum_{t=0}^{\infty}R(s_t)|\pi , s_0 = s']
         </div>
         <p>
          Ultimately, the result is the following.
         </p>
         <div class="mathquill">
          U^{\pi}(s) = R(s) + U^{\pi}(s')
         </div>
         <p>
          As you see here, calculating the utility of a state is an iterative process. It involves all of the states that the agent would visit between the present state and the goal, as dictated by the policy.
         </p>
         <p>
          As well, it should be clear that the utility of a state depends on the policy. If you change the policy, the utility of each state will change, since the sequence of states that would be visited prior to the goal may change.
         </p>
         <h2 id="determining-the-optimal-policy">
          Determining the Optimal Policy
         </h2>
         <p>
          Recall that the
          <strong>
           optimal policy
          </strong>
          , denoted
          <span class="mathquill ud-math">
           \pi^*
          </span>
          , informs the robot of the
          <em>
           best
          </em>
          action to take from any state, to maximize the overall reward. That is,
         </p>
         <div class="mathquill">
          \pi^*(s) = \underset{a}{argmax} E [U^{\pi}(s)]
         </div>
         <p>
          In a state
          <span class="mathquill ud-math">
           s
          </span>
          , the optimal policy
          <span class="mathquill ud-math">
           \pi^*
          </span>
          will choose the action
          <span class="mathquill ud-math">
           a
          </span>
          that maximizes the utility of
          <span class="mathquill ud-math">
           s
          </span>
          (which, due to its iterative nature, maximizes the utilities of all future states too).
         </p>
         <p>
          While the math may make it seem intimidating, it’s as easy as looking at the set of actions and choosing the best action for every state. The image below displays the set of all actions once more.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/mdpimage-73-v2.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          It may not be clear from the get-go which action is optimal for every state, especially for states far away from the goal which have many paths available to them. It’s often helpful to start at the goal and work your way backwards.
         </p>
         <p>
          If you look at the two cells adjacent to the goal, their best action is trivial - go to the goal! Recall from your learning in RL that the goal state’s utility is 0. This is because if the agent starts at the goal, the task is complete and no reward is received. Thus, the expected reward from either of the goal’s adjacent cells is 79.8. Therefore, the state’s utility is, 79.8 + 0 = 79.8 (based on
          <span class="mathquill ud-math">
           U^{\pi}(s) = R(s) + U^{\pi}(s')
          </span>
          ).
         </p>
         <p>
          If we look at the lower mountain cell, it is also easy to guess which action should be performed in this state. With an expected reward of -1.2, moving right is going to be much more rewarding than taking any indirect route (up or left). This state will have a utility of -1.2 + 79.8 = 78.6.
         </p>
         <p>
          Now it's your turn!
         </p>
         <h2 id="quiz">
          Quiz
         </h2>
         <p>
          Can you calculate what would the utility of the state to the right of the center mountain be, if the most rewarding action is chosen?
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/mdpimage-74-v2.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <div>
          <p>
           <strong>
            QUESTION:
           </strong>
           <p>
            What is the utility of the state to the right of the center mountain following the optimal policy?
           </p>
          </p>
          <div class="" form-group"="">
           <label for="answer">
            <strong>
             ANSWER:
            </strong>
           </label>
           <textarea class="form-control" id="answer"></textarea>
          </div>
         </div>
         <details>
          <summary>
           <strong>
            SOLUTION:
           </strong>
          </summary>
          <p>
           <i>
            NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer
           </i>
          </p>
         </details>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The process of selecting each state’s most rewarding action continues, until every state is mapped to an action. These mappings are precisely what make up the policy.
         </p>
         <p>
          It is highly suggested that you pause this lesson here, and work out the optimal policy on your own using the action set seen above. Working through the example yourself will give you a better understanding of the challenges that are faced in the process, and will help you remember this content more effectively. When you are done, you can compare your results with the images below.
         </p>
         <h2 id="applying-the-policy">
          Applying the Policy
         </h2>
         <p>
          Once this process is complete, the agent (our robot) will be able to make the best path planning decision from every state, and successfully navigate the environment from any start position to the goal. The optimal policy for this environment and this robot is provided below.
         </p>
         <p>
          The image below that shows the set of actions with just the optimal actions remaining. Note that from the top left cell, the agent could either go down or right, as both options have equal rewards.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/mdpimage-47-v1.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/mdpimage-53-v2.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="discounting">
          Discounting
         </h2>
         <p>
          One simplification that you may have noticed us make, is omit the discounting rate
          <span class="mathquill ud-math">
           \gamma
          </span>
          . In the above example,
          <span class="mathquill ud-math">
           \gamma = 1
          </span>
          and all future actions were considered to be just as significant as the present action. This was done solely to simplify the example. After all, you have already been introduced to
          <span class="mathquill ud-math">
           \gamma
          </span>
          through the lessons on Reinforcement Learning.
         </p>
         <p>
          In reality, discounting is often applied in robotic path planning, since the future can be quite uncertain. The complete equation for the utility of a state is provided below:
         </p>
         <div class="mathquill">
          U^{\pi}(s) = E[\sum_{t=0}^{\infty}\gamma^tR(s_t)|\pi , s_0 = s]
         </div>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="14. Value Iteration Algorithm.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('13. State Utility')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
