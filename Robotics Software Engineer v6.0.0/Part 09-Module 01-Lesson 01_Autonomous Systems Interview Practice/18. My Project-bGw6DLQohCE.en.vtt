WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.220
So, Michael, I wanted to thank you again for coming in today.

00:00:02.220 --> 00:00:03.790
I had a chance to review your resume,

00:00:03.790 --> 00:00:04.969
and I was actually quite impressed.

00:00:04.969 --> 00:00:07.049
So, you have a background in accounting and you've actually

00:00:07.049 --> 00:00:10.019
transition doing more autonomous systems type work recently.

00:00:10.019 --> 00:00:10.309
Yes.

00:00:10.310 --> 00:00:12.870
So, there's one project in particular be really interested in hearing about,

00:00:12.869 --> 00:00:16.500
which was using convolutional neural nets in self-driving cars.

00:00:16.500 --> 00:00:20.190
I was wondering if you wouldn't mind taking me through a little bit of that project,

00:00:20.190 --> 00:00:22.214
some of the challenges that you faced,

00:00:22.214 --> 00:00:25.019
some of the methods that you used and if there's

00:00:25.019 --> 00:00:26.550
anything that you would have liked to have done

00:00:26.550 --> 00:00:28.195
that you just didn't have a chance to yet.

00:00:28.195 --> 00:00:30.375
Sure. So, in that project,

00:00:30.375 --> 00:00:34.189
I was originally attempting to come up with a deep learning approach that was more

00:00:34.189 --> 00:00:38.434
robust than what we had done in the class around advanced lane finding.

00:00:38.435 --> 00:00:39.975
So, in the project,

00:00:39.975 --> 00:00:43.725
we had used more classic computer vision techniques,

00:00:43.725 --> 00:00:46.179
where we use different colored gradients, et cetera,

00:00:46.179 --> 00:00:50.130
to go through and find where the ley lines like they were in the image,

00:00:50.130 --> 00:00:52.155
then we had to do perspective transformations,

00:00:52.155 --> 00:00:55.730
where we essentially got a bird's eye view of the road.

00:00:55.729 --> 00:00:58.619
All this took quite a bit of computational power,

00:00:58.619 --> 00:01:00.369
so it wasn't really real time.

00:01:00.369 --> 00:01:04.030
I believe it was running at roughly three FPS or so,

00:01:04.030 --> 00:01:06.780
which doesn't really work in real-world applications.

00:01:06.780 --> 00:01:08.275
You're not updating fast enough.

00:01:08.275 --> 00:01:10.850
So, in this approach, I wanted to get something that truly could

00:01:10.849 --> 00:01:13.879
go at real speed around 30 FPS or so,

00:01:13.879 --> 00:01:16.649
as well as being more robust than what I did before.

00:01:16.650 --> 00:01:19.640
The original one wasn't very good on curves for instance.

00:01:19.640 --> 00:01:22.215
Once they got up to too big of a curve,

00:01:22.215 --> 00:01:24.109
the algorithm just would stop working,

00:01:24.109 --> 00:01:25.875
and run off the page essentially,

00:01:25.875 --> 00:01:28.965
and think that the lines were going straight when they're really going curved.

00:01:28.965 --> 00:01:32.984
So, along with gathering that training data then,

00:01:32.984 --> 00:01:35.790
which I went out in the sense you just drove around my car.

00:01:35.790 --> 00:01:38.555
I had my phone up and taking in

00:01:38.555 --> 00:01:42.250
video images and extracting those frames out from my training data.

00:01:42.250 --> 00:01:44.165
I then moved on to my network.

00:01:44.165 --> 00:01:48.180
My network was built using a similar architecture to SegNet,

00:01:48.180 --> 00:01:49.725
which was a paper I read at the time.

00:01:49.724 --> 00:01:51.189
I'm putting it into a diagram it out here.

00:01:51.189 --> 00:01:52.215
Yeah. Could you show me that really quick?

00:01:52.215 --> 00:01:54.040
I'm not really familiar with that one.

00:01:54.040 --> 00:01:56.590
So, SegNet's basic idea,

00:01:56.590 --> 00:01:59.975
it starts out like a normal convolutional neural network.

00:01:59.974 --> 00:02:03.919
Usually, a convolutional neural network comes out

00:02:03.920 --> 00:02:07.290
with an input image coming in as your input.

00:02:07.290 --> 00:02:12.719
Then, you have successive layers that are smaller convolutional layers.

00:02:13.680 --> 00:02:16.665
A lot of times in another network,

00:02:16.664 --> 00:02:18.709
you might have a fully connected layer at this point.

00:02:18.710 --> 00:02:21.730
But the difference with a fully convolutional neural network,

00:02:21.729 --> 00:02:24.709
which is what SegNet had in it was that on the flip side,

00:02:24.710 --> 00:02:28.115
you come back with what are called D-convolutional layers that

00:02:28.115 --> 00:02:32.000
solely get larger and have this semantic output,

00:02:32.000 --> 00:02:34.889
where it potentially is the same size.

00:02:34.889 --> 00:02:37.049
Your imagery could resize it back up to that.

00:02:37.050 --> 00:02:40.630
But every single pixel in that output is actually a different class.

00:02:40.629 --> 00:02:44.030
In my case, it was whether you were the lane, or not the lane.

00:02:44.030 --> 00:02:45.770
So, in doing so,

00:02:45.770 --> 00:02:49.085
then I'm basically doing binary classification between the two.

00:02:49.085 --> 00:02:51.510
There's also applications where you have more cources,

00:02:51.509 --> 00:02:54.125
say, a vehicle, or pedestrian, et cetera.

00:02:54.125 --> 00:02:58.080
In my case, though I was just wanting to improve on my lanefining algorithm.

00:02:58.080 --> 00:02:59.800
So, I just looked at the lane itself.

00:02:59.800 --> 00:03:02.830
Some of the other applications also look for just free space on the road,

00:03:02.830 --> 00:03:04.115
which I thought, well,

00:03:04.115 --> 00:03:06.800
I don't really want my car to be driving on the opposite side of the road.

00:03:06.800 --> 00:03:09.860
So, I really only want to know what my own lane is here.

00:03:09.860 --> 00:03:13.700
One of the challenges that I ran into first

00:03:13.699 --> 00:03:19.879
was in actually getting this to fully work in train correctly.

00:03:19.879 --> 00:03:23.030
Because semantic segmentation networks

00:03:23.030 --> 00:03:25.610
actually also use a technique that are called skip layers.

00:03:25.610 --> 00:03:29.690
Now, skip layers can very easily be used to

00:03:29.689 --> 00:03:33.939
essentially go from skipping entire middle section of the network,

00:03:33.939 --> 00:03:36.689
or they could also be used just to skip between

00:03:36.689 --> 00:03:40.085
two layers or more layers, however many you want.

00:03:40.085 --> 00:03:44.270
Now, this is important because when you've extracted some more advanced features,

00:03:44.270 --> 00:03:46.310
but you've also gone through max-pooling layers,

00:03:46.310 --> 00:03:48.289
you have a lot of times losing information about

00:03:48.289 --> 00:03:50.379
where in the image a certain thing occurred.

00:03:50.379 --> 00:03:52.289
So, if I'm trying to classify every pixel,

00:03:52.289 --> 00:03:54.669
it's important to have that information at the end,

00:03:54.669 --> 00:03:57.155
and be able to draw that back.

00:03:57.155 --> 00:04:02.125
Now, another thing that happens is when you perform backpropagation,

00:04:02.125 --> 00:04:04.490
you need to make sure that the information is actually

00:04:04.490 --> 00:04:07.295
going all the way through because if you get a really deep network,

00:04:07.294 --> 00:04:11.359
backpropagation can have what's called a vanishing gradient problem,

00:04:11.360 --> 00:04:14.120
where you're doing so much of the chain rule,

00:04:14.120 --> 00:04:15.830
and multiplication going back.

00:04:15.830 --> 00:04:18.500
Then it starts diminishing that gradient and, essentially,

00:04:18.500 --> 00:04:23.225
you don't really feed in the error to those earlier layers in a really deep network.

00:04:23.225 --> 00:04:25.820
This helps fix that problem because you

00:04:25.819 --> 00:04:29.180
actually skip in over missing some of the multiplication in the middle,

00:04:29.180 --> 00:04:33.129
and that backpropagation signal continues all the way back to the front of the network,.

00:04:33.129 --> 00:04:36.000
Really interesting. Nice. Thank you.

00:04:36.000 --> 00:04:37.160
Yeah. Thanks.

