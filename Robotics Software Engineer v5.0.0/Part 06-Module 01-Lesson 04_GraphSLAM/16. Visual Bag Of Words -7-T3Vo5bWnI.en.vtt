WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.710
In our tab mapping,

00:00:01.710 --> 00:00:04.988
loop closure is detected using a bag-of-words approach.

00:00:04.988 --> 00:00:08.580
Bag-of-words is commonly used in vision-based mapping.

00:00:08.580 --> 00:00:11.580
A feature is a very specific characteristic of

00:00:11.580 --> 00:00:14.160
an image like a patch with a complex texture,

00:00:14.160 --> 00:00:16.605
or a well-defined edge or corner.

00:00:16.605 --> 00:00:20.129
In our tab map, the default method for extracting features from

00:00:20.129 --> 00:00:23.929
an image is called Speeded Up Robust Features or SURF.

00:00:23.929 --> 00:00:27.265
Each feature has a descriptor associated with it.

00:00:27.265 --> 00:00:28.929
A feature descriptor is

00:00:28.929 --> 00:00:32.784
a unique and robust representation of the pixels that make up a feature.

00:00:32.784 --> 00:00:35.679
In SURF, the point of interests where the feature is

00:00:35.679 --> 00:00:39.009
located is split into smaller square sub-regions.

00:00:39.009 --> 00:00:42.549
From these sub-regions, the pixel intensities in regions

00:00:42.549 --> 00:00:46.344
of regularly spaced sample points are calculated and compared.

00:00:46.344 --> 00:00:48.939
The differences between the sample points are used

00:00:48.939 --> 00:00:51.714
to categorize the sub-regions of the image.

00:00:51.715 --> 00:00:55.310
Comparing feature descriptors directly is time consuming,

00:00:55.310 --> 00:00:58.664
so vocabulary is used for faster comparison.

00:00:58.664 --> 00:01:03.104
This is where similar features or synonyms are clustered together.

00:01:03.104 --> 00:01:07.394
The collection of these clusters represent the vocabulary.

00:01:07.394 --> 00:01:10.920
One of feature descriptor is mapped to one and the vocabulary,

00:01:10.920 --> 00:01:12.900
it is called quantization.

00:01:12.900 --> 00:01:15.359
At this point, the feature is now linked to

00:01:15.359 --> 00:01:18.704
a word and can be referred to as a visual word.

00:01:18.704 --> 00:01:21.480
When all features in an image are quantized,

00:01:21.480 --> 00:01:23.685
the image is now a bag-of-words.

00:01:23.685 --> 00:01:27.420
Each word keeps a link to images that it is associated with,

00:01:27.420 --> 00:01:31.650
making image retrieval more efficient over a large data set.

00:01:31.650 --> 00:01:34.620
To compare an image with our previous images,

00:01:34.620 --> 00:01:38.700
a matching score is given to all images containing the same words.

00:01:38.700 --> 00:01:41.880
Each word keeps track of which image it has been seen in,

00:01:41.879 --> 00:01:43.964
so similar images can be found.

00:01:43.965 --> 00:01:46.530
This is called an inverted index.

00:01:46.530 --> 00:01:48.704
If a word is seen in an image,

00:01:48.704 --> 00:01:50.954
the score of this image will increase.

00:01:50.954 --> 00:01:56.099
If an image shares many visual words with the query image, it will score higher.

00:01:56.099 --> 00:01:59.489
A Bayesian filter is used to evaluate the scores.

00:01:59.489 --> 00:02:02.939
This is the hypothesis that an image has been seen before.

00:02:02.939 --> 00:02:06.420
When the hypothesis reaches a predefined threshold H,

00:02:06.420 --> 00:02:08.560
a loop closure is detected.

